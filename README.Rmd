---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# tokenvars

<!-- badges: start -->
<!-- badges: end -->

At the moment, this package is super experimental and cannot be considered easy to use. Even when it is the case, this is mostly an infrastructural R package for a very niche category of developers wanting to develop R packages for [quanteda](https://github.com/quanteda/quanteda).

`quanteda` has good support for metadata. However, one can only put corpus- and document-level metadata (`meta()`, `docvars()`, respectively). This package aims at going down one level and provides support for token-level metadata. Token-level metadata is useful for tagging individual token (e.g. Parts of Speech, relationships among tokens); it is also useful to store upper-level information of tokens (e.g. the subword tokenized sequence of tokens "\_L", "'", "app", "ar", "tement"; you might want to know "\_L" is from the French word "*L'appartement*").

This is a generalization of the approach used in [quanteda.proximity](https://github.com/gesistsa/quanteda.proximity) for recording token-level metadata.

## Installation

You can install the development version of tokenvars like so:

``` r
# Well, if you don't know how to do this, you probably shouldn't try this.
```

## Proof of concept

This is a basic example which shows you how to solve a common problem:

```{r example}
library(quanteda)
library(tokenvars)

corp <- corpus(c(d1 = "spaCy is great at fast natural language processing.",
                 d2 = "Mr. Smith spent two years in North Carolina."))

tok <- tokens(corp) %>% tokens_add_tokenvars()
tok
```

```{r example2}
tokenvars(tok) ## nothing to see here
```

```{r example3}
tokenvars(tok, "tag") <- list(c("NNP", "VBZ", "JJ", "IN", "JJ", "JJ", "NN", "NN", "."),
                              c("NNP", ".", "NNP", "VBD", "CD", "NNS", "IN", "NNP", "NNP", "."))
tokenvars(tok, "lemma") <- list(c("spaCy", "be", "great", "at", "fast", "natural", "language", "processing", "."),
                                c("Mr", ".", "Smith", "spend", "two", "year", "in", "North", "Carolina", "."))
```

```{r example4}
tok
```

```{r example5}
tokenvars(tok)
```

```{r example6}
tokenvars(tok, field = "tag")
```

```{r example7}
tokenvars(tok, field = "lemma", docnames = "d2")
```
